{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (f)MRI processing using Nilearn\n",
    "\n",
    "*IMPRS - Using Python for Cognitive Science (2023). This tutorial is based on several [Nilearn](https://nilearn.github.io/stable/index.html) tutorials and work by [Lukas Snoek](https://lukas-snoek.com/)*\n",
    "\n",
    "Good morning! Today we will be working through this tutorial to get familiar with the processing of (BOLD)MRI data in Python using *Nilearn*. Nilearn is a package that provides tools for analyses such as functional connectivity, multivariate (machine-learning based) decoding, but also more basic tools like image manipulation and visualization.  \n",
    "\n",
    "At the end of this tutorial you'll know\n",
    "- How to read in (BOLD)MRI data\n",
    "- How to visualize brain images (make pretty brain plots)\n",
    "- How to manipulate brain image volumes (resampling, smoothing, masking, ROIs)\n",
    "- How to extract data from regions of interest\n",
    "- (if time permits) how to implement connectivity analyses \n",
    "\n",
    "Disclaimer: This tutorial is intended to increase your Python skills while working with fMRI data and does not cover all fMRI preprocessing steps (!). It also does not cover the statistical and machine-learning tools that Nilearn provides. \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's start again by \n",
    "1. Making a virtual environment for this week's session. For this, open a terminal in the right folder and type `python -m venv .venv`\n",
    "2. Open a new terminal (make sure you're in the virtual environment) and install the necesssary packages using  `pip install -r requirements.txt` \n",
    "3. In this notebook, select your virtual environment at \"select kernel'\n",
    "\n",
    "All done? Then you're ready to start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Importing modules\n",
    "\n",
    "Today, we will need os, NumPy, matplotlib, and some modules from nilearn (`datasets`, `plotting`, `image`)\n",
    "\n",
    "<font color='green'>**Exercise 1:**</font> Import them below. We have added matplotlib for you already.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### your code here\n",
    "# from nilearn import \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading in data\n",
    "\n",
    "#### 1.1 File format \n",
    "\n",
    "Nilearn's functionality implicitly assumes that your MRI data is stored in nifti images. In nilearn, they often use the term “niimg” as abbreviation that denotes either a file name or a NiftiImage object (Nilearn can operate on either file names or NiftiImage objects). Niimgs can be 3D or 4D. A 4D niimg may for instance represent a time series of 3D images. It can be a list of file names, if these contain 3D information. For more info about data formats in Nilearn, see [here](https://nilearn.github.io/manipulating_images/input_output.html).\n",
    "\n",
    "Let's now get an example dataset. There are many datasets available on openneuro, osf etc. If you want, feel free to get a dataset (one participant) that you find interesting! Note that different datasets are often saved in different ways and formats (which system was used, how files were exported etc), so you might run in to some 'problems' later in this tutorial (but we are happy to help with those issues). Fortunately, Nilearn provides dataset fetching function that automatically downloads reference datasets and atlases. They can be imported from [`nilearn.datasets`](https://nilearn.github.io/modules/reference.html#module-nilearn.datasets)\n",
    "\n",
    "Here we will use [`nilearn.datasets.fetch_haxby`](https://nilearn.github.io/modules/generated/nilearn.datasets.fetch_haxby.html) to load the data from this paper by [Haxby et al (2001)](https://www.science.org/doi/10.1126/science.1063736). This dataset contains results from a classical fMRI study that investigated the differences between the neural correlates of face versus object processing in the ventral visual stream. Face and object stimuli showed widely distributed and overlapping\n",
    "response patterns:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data using the default settings (note that this may take a while):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haxby_dataset = datasets.fetch_haxby()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>**Exercise 2:**</font> While the data is downloading, have a look at the Nilearn website to see what is possible or have a look at the Haxby paper to see what data we're dealing with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching the Haxby dataset using `fetch_haxby()` returns a data structure that contains different pieces of information on the retrieved dataset, including the file names on hard disk:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haxby_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the folder on your computer and check out the different files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>**Exercise 3:**</font> Why was the data from subject 2 downloaded? How would you download the data from another participant? (Hint: look at the fetch_haxby function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  You can try it out, but don't worry if it takes too long (just think about it!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation and further resources of the dataset at hand can be retrieved as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(haxby_dataset.description)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this Haxby dataset has an anatomical image, functional images and masks. Let's load the anatomical and functional data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_filename = haxby_dataset.func[0]\n",
    "anat_filename = haxby_dataset.anat[0]\n",
    "\n",
    "# print basic information on the dataset\n",
    "print('First subject functional nifti images (4D) are at: %s' %\n",
    "      fmri_filename)  # 4D data\n",
    "print('First subject anatomical nifti images (3D) are at: %s' %\n",
    "      anat_filename)  # 3D data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Headers\n",
    "\n",
    "Beyond the values of each voxel in the brain data, it is also critical to store other information about the image, known generally as metadata. These data are generally stored in a header, which can either be a separate file or a part of the image file. The information in the header will differ between different image formats. For example, the header information for a NIfTI1 format file differs from the header information for a MINC or DICOM format file.\n",
    "\n",
    "Our image is a NIfTI1 format image, and it therefore has a NIfTI1 format header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_func = nib.load(fmri_filename)\n",
    "img_anat = nib.load(anat_filename)\n",
    "\n",
    "## get the header of the functional scan: \n",
    "header = img_func.header\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The header of any image will normally have the following methods:\n",
    "\n",
    "- `get_data_shape()` to get the output shape of the image data array:\n",
    "- `get_data_dtype()` to get the numpy data type in which the image data is stored (or will be stored if you save the image):\n",
    "- `get_zooms()` to get the voxel sizes in millimeters: The fourth value of header.get_zooms() is the time between scans in milliseconds; this is the equivalent of voxel size on the time axis.\n",
    "\n",
    "<font color='green'>**Exercise 4:**</font> For the functional data, get the voxel sizes in millimeters and the time between scans in milliseconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>**Exercise 5:**</font> Now do the same for the anatomical scan! What differences do you notice? Why is this the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here, note that you first need to get the header from anat_filename\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Visualization of brain images\n",
    "\n",
    "Nilearn has several plotting tools, such as `plotting.plot_anat` for plotting anatomical images (https://nilearn.github.io/stable/auto_examples/01_plotting/plot_demo_plotting.html#sphx-glr-auto-examples-01-plotting-plot-demo-plotting-py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_anat(anat_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to alter the default annotations of plots, using the method `annotate` of the display objects. For example, we can add a scale bar at the bottom right of each view and indicate which coordinates we want to see:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = plotting.plot_anat(anat_filename,\n",
    "                             title=\"adding a scale bar\",\n",
    "                             cut_coords=[-34, -39, -9])\n",
    "display.annotate(scalebar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nilearn plotting functions contain many (optional) arguments that you can use to customize your plot. For example, the display_mode argument allows you to plot the image in one (or more) particular dimensions (e.g., the \"X\" axis, which is usually sagittal) and the cut_coords argument allows you to specify the number (if integer) or locations of the particular slices/cuts (if list):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = plotting.plot_anat(anat_filename, display_mode='x', cut_coords=[-40, -20, 0, 20, 40])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks nice! But what if we want to show certain slices from the different dimensions in one plot? We can write our own function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## here we use nibabel to load the data into an array\n",
    "import nibabel as nib\n",
    "anat_img_data = nib.load(anat_filename).get_fdata()\n",
    "\n",
    "def show_slices(slices):\n",
    "    \"\"\" Function to display row of image slices \"\"\"\n",
    "    fig, axes = plt.subplots(1, len(slices))\n",
    "    for i, slice in enumerate(slices):\n",
    "        axes[i].imshow(slice.T, cmap=\"gray\", origin=\"lower\")\n",
    "\n",
    "slice_0 = anat_img_data[28, :, :]\n",
    "slice_1 = anat_img_data[:, 83, :]\n",
    "slice_2 = anat_img_data[:, :, 118]\n",
    "slice_3 = anat_img_data[44, :, :]\n",
    "show_slices([slice_0, slice_1, slice_2, slice_3])\n",
    "plt.suptitle(\"Some slices for anatomical image\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `plotting.plot_roi` we can visualize specific regions of interest (ROIs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_filename = haxby_dataset.mask_house[0]\n",
    "\n",
    "plotting.plot_roi(mask_filename, bg_img=anat_filename,\n",
    "                  title=\"plot_roi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='purple'>**To think:**</font> What ROI did we plot?\n",
    "\n",
    "\n",
    "Another tool to plot different slices of a given image is `plotting.plot_img`.\n",
    "\n",
    "<font color='green'>**Exercise 6:**</font> Use plot_img to visualize the anatomical scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nilearn has a whole section on plotting, with a nice example gallery: https://nilearn.github.io/plotting/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Manipulating brain image volumes\n",
    "\n",
    "Nilearn also contains a lot of functionality to easily manipulate images. Note that (like we also saw last week) all of this could be implemented with numpy (in fact, Nilearn uses numpy \"under the hood\" for many of its operations); just see the Nilearn functions as \"wrappers\" around common numpy routines for nifti-based arrays (which also handle the updating of image affines, e.g. after resampling an image, appropriately).\n",
    "\n",
    "To showcase some nilearn functions, we'll use the functional MRI data we loaded before (img_func).\n",
    "\n",
    "#### 3.1 A better look at functional data\n",
    "Let's print the shape of our functional data file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of functional MRI image: %s\" % (img_func.shape,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file, however, is quite large, as it contains concatenated data across 12 runs. To limit the amount of data that we need to load into RAM, we'll only load in the data from the first run. We can find out which volumes belong to which run in the \"session_target\" information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haxby_dataset.session_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load this file as a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "metadata = pd.read_csv(haxby_dataset['session_target'][0], sep=' ')\n",
    "print(\"Shape of metadata dataframe: %s\" % (metadata.shape,), end='\\n\\n')\n",
    "metadata.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, \"chunks\" refer to the specific run index and \"labels\" refers to the timepoint-by-timepoint condition (i.e., the condition of the active block at each moment in time, e.g., images of scissors, images of chairs, rest blocks, etc.). Let's compute the number of timepoints in the first \"chunk\" (run):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvol_run_1 = np.sum(metadata['chunks'] == 0)\n",
    "print(\"Number of timepoints in run 1: %i\" % nvol_run_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now we can use Nilearn's `index_img` function from the image module to index our func_img object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_index = np.arange(nvol_run_1, dtype=int)\n",
    "img_func_run1 = image.index_img(img_func, to_index)\n",
    "print(\"Shape of func_img_run1: %s\" % (img_func_run1.shape,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>**Exercise 7:**</font> With the information in the header from img_func_run1, can you determine how long (in seconds) the first run lasted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Spatial processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nilearn also provides some functionality for basic preprocessing of (functional) images. Note the adjective \"basic\" — most preprocessing steps (such as image registration, motion correction, susceptibility distortion correction, etc.) should be done using other packages (for which we strongly recommend [Fmriprep](https://fmriprep.org/en/stable/?msclkid=e31c4af9d0f811ec86025fc6ef658a67))). That said, Nilearn does provide some preprocessing functionality, such as smoothing (with image.smooth_img).\n",
    "\n",
    "*Why smoothing?* \n",
    "Functional MRI data have a low signal-to-noise ratio. When using methods that are not robust to noise, it is useful to apply a spatial filtering kernel on the data. Such data smoothing is usually applied using a Gaussian function with 4mm to 12mm full-width at half-maximum (this is where the FWHM comes from). The function nilearn.image.smooth_img accounts for potential anisotropy in the image affine (i.e., non-indentical voxel size in all the three dimensions). \n",
    "\n",
    "\n",
    "Alright, before we look at smoothing, let's make our data a little simpler. \n",
    "Instead of looking at the functional data over time, let's compute the mean across time for every voxel: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the voxel_wise mean of functional images across time.\n",
    "# Basically reducing the functional image from 4D to 3D\n",
    "mean_haxby_img = image.mean_img(img_func_run1)\n",
    "\n",
    "# Visualizing mean image (3D)\n",
    "display = plotting.plot_epi(mean_haxby_img, title=\"plot_epi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, let's see what happens if we add some smoothing! For this, we can use `image.smooth_img`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_smooth = image.smooth_img(mean_haxby_img, fwhm=5)\n",
    "display = plotting.plot_epi(func_smooth, title = \"plot_epi_smooth\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>**Exercise 8:**</font> Try out different smoothing values and plot below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### interactive plotting (let's try this time) \n",
    "Note that you can even create interactive image viewers using, for example, the view_img function. Importantly (at least in Jupyter notebooks), you should call this plotting function at the end of the code cell and you should not assign the output of the function to a new variable, otherwise the viewer won't render."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we can even add our anatomical image as a background image! and add an optional threshold argument:\n",
    "plotting.view_img(func_smooth, bg_img=img_anat, threshold=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>**Exercise 9:**</font> Try moving the crosshairs! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our data is aligned to standard MNI152 space (an \"average brain\", generated by taking 152 brains and coregistering them), we can use another cool visualization feature from Nilearn: surface plots! Nilearn contains the resampling parameters (as available from the Freesurfer software package) necessary to resample volumetric images in MNI space to the corresponding \"fsaverage\" surface space. To do so, you can use the view_img_on_surf function, which takes a volumetric image (img) and projects it on a corresponding surface (surf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_img_on_surf(\n",
    "    stat_map_img=func_smooth,\n",
    "    surf_mesh='fsaverage5'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out different views of the surface by dragging it left/right/up/down with your mouse and zooming in (with the scroll wheel of your mouse or your trackpad). Also try switching hemispheres and surface type (Inflated vs. Pial) using the buttons on the bottom of the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**While these interactive viewers are amazing, be careful not to open too many of them, as they need quite a bit of memory to run!**\n",
    "\n",
    "#### 3.3 Extracting a brain mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common operation in fMRI analyses is masking: extracting particular voxels from the entire dataset, usually based on a binary brain mask (like you computed in the previous ToDo). Masking, at least in fMRI analyses, is often done on the spatial dimensions of 4D images. Reasons to mask your data could be, for example, to exclude non-brain voxels (like in skullstripping) or to perform confirmatory region-of-interest (ROI) analyses, or to extract one or multiple \"seed regions\" for connectivity analyses.\n",
    "\n",
    "Let's compute a mask based on our 3D functional image (the mean of our functional data) and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.masking import compute_epi_mask\n",
    "mask_img = compute_epi_mask(img_func_run1)\n",
    "\n",
    "# Visualize it as an ROI\n",
    "from nilearn.plotting import plot_roi\n",
    "plot_roi(mask_img, mean_haxby_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Applying the mask to extract the corresponding time series\n",
    "These are some really lovely images, but we're also interested in the timeseries in these different voxels! To transform our Nifti images into matrices, we’ll use Nilearn's `apply_mask` to extract the fMRI data from a mask and convert it to data series. The shape of our resulting masked_data is (timepoints, voxels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.masking import apply_mask\n",
    "masked_data = apply_mask(img_func_run1, mask_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(masked_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display this time series to get an intuition of how the whole brain signal is changing over time. We can plot the first 150 timepoints from ten voxels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now plot a few of these\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(masked_data[:150, :10])\n",
    "plt.xlabel('Time [TRs / scan number]', fontsize=16)\n",
    "plt.ylabel('Intensity', fontsize=16)\n",
    "plt.xlim(0, 120)\n",
    "plt.subplots_adjust(bottom=.12, top=.95, right=.95, left=.12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Temporal processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have our timeseries! Nilearn contains some basic temporal preprocessing functionality, such as detrending (removing a linear trend); standardization (normalizing the signal with mean 0 and standard deviation 1); high- and low-pass filtering; generic confound removal (using linear regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, there are different ways to perform these operations. The most \"basic\" ones are `signal.clean` and `image.clean_img`, which are very similar except for that `signal.clean` works on 2D numpy arrays while `image.clean_img` work on 4D \"niimg-like\" objects.\n",
    "\n",
    "https://nilearn.github.io/modules/generated/nilearn.image.clean_img.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean our data a bit. For example, we can use a highpass filter to get rid of very slow drifts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_clean = image.clean_img(img_func_run1, runs=None, detrend=False, \n",
    "                             standardize=False, confounds=None, low_pass=None, \n",
    "                             high_pass=0.01, t_r=2.5, ensure_finite=False, \n",
    "                             mask_img=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_data = apply_mask(func_clean, mask_img)\n",
    "# And now plot a few of these\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(masked_data[:150, :10])\n",
    "plt.xlabel('Time [TRs / scan number]', fontsize=16)\n",
    "plt.ylabel('Intensity', fontsize=16)\n",
    "plt.xlim(0, 120)\n",
    "plt.subplots_adjust(bottom=.12, top=.95, right=.95, left=.12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>**Exercise 10:**</font> Detrend and standardize the data and plot the results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Extracting timeseries from different ROIs\n",
    "\n",
    "Now that we’ve seen how to create a data series from different voxels, we can start to scale up! What if, instead of wanting to extract signal from one voxel, we want to define several ROIs and extract signal from all of them? Nilearn can help us with that, too. For this, we’ll use nilearn.input_data.NiftiLabelsMasker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's load a parcellation (brain atlas) that we'd like to use\n",
    "multiscale = datasets.fetch_atlas_basc_multiscale_2015(resume=True)\n",
    "print('Atlas ROIs are located at: %s' % multiscale.scale064)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For different atlases see: https://nilearn.github.io/stable/modules/reference.html#atlases \n",
    "\n",
    "`datasets.fetch_atlas_basc_multiscale_2015` downloads and loads multiscale functional brain parcellations. The atlas includes group brain parcellations generated from resting-state functional MRI from about 200 young healthy subjects. Multiple scales are available (how many regions you want), let's have a look at the scale-36:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_roi(multiscale.scale036)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay! Using `NifitLabelsMaker` with this atlas will help us to extract the signal averaged across voxels in our different regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.input_data import NiftiLabelsMasker, NiftiMasker\n",
    "label_masker = NiftiLabelsMasker(labels_img=multiscale.scale036, standardize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_masker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_matrix = label_masker.fit_transform(func_clean)\n",
    "print(fmri_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fmri_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Connectivity analyses\n",
    "\n",
    "Now that we have a matrix containing the timeseries for different regions of interest, we can create a connectome. A connectome is a map of the connections in the brain. Since we’re working with functional data, however, we don’t have access to actual connections. Instead, we’ll use a measure of statistical dependency to infer the (possible) presence of a connection.\n",
    "\n",
    "Here, we’ll use Pearson’s correlation as our measure of statistical dependency and compare how all of our ROIs from our chosen parcellation relate to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import connectome\n",
    "correlation_measure = connectome.ConnectivityMeasure(kind='correlation')\n",
    "correlation_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = correlation_measure.fit_transform([fmri_matrix])\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "correlation_matrix = correlation_matrix[0]\n",
    "# Mask the main diagonal for visualization:\n",
    "# np.fill_diagonal(correlation_matrix, 0)\n",
    "plotting.plot_matrix(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>**Exercise 11/To think:**</font> What do the X and Y axis represent?\n",
    "\n",
    "It can also be helpful to project these connection weightings back on to the brain, to visualize these connectomes! Let's try. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find coordinates of different regions \n",
    "coords = plotting.find_parcellation_cut_coords(multiscale.scale036)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive plotting (if it works)\n",
    "view = plotting.view_connectome(correlation_matrix, node_coords = coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot connectome with 80% edge strength in the connectivity\n",
    "plotting.plot_connectome(correlation_matrix, coords,\n",
    "                         edge_threshold=\"80%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='orange'>**Homework assignment**</font> \n",
    "\n",
    "1. Plot the activity from run 2, from ten randomly chosen voxels. \n",
    "\n",
    "2. Extract the averaged timeseries from different regions of interest and plot the timeseries (you can use a different scale from the atlas we loaded, or use a completely different atlas) \n",
    "\n",
    "3. Can you write a loop that plots the averaged functional data with varying amounts of smoothing, from no smoothing to 20mm smoothing, in steps of 5mm? \n",
    "\n",
    "<font color='purple'>*Bonus:*</font> Can you add the amount of smoothing to the title of the plot?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to know out more? see \n",
    "- https://peerherholz.github.io/workshop_weizmann/data/image_manipulation_nilearn.html?msclkid=9d87cfd3cf8a11ecb5e0967fbfd32eb6\n",
    "- https://emdupre.github.io/nha2020-nilearn/01-data-structures.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f709d7cb232b41ce914599eec0a9902e5af303746b437c586166f6af9a54b8c7"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
